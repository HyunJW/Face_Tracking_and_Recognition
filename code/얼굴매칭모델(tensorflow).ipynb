{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv2D, Dense, Flatten,MaxPooling2D, Dropout\n",
    "from keras.layers import Lambda, Subtract\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    training_dir = \"D:/data/training/\"\n",
    "    testing_dir = \"D:/data/testing/\"\n",
    "    validation_dir = \"D:/data/validation/\"\n",
    "    train_batch_size = 64\n",
    "    train_number_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMiniBatch(batch_size=Config.train_batch_size, path=Config.training_dir, prob=0.5):\n",
    "    persons = os.listdir(path)\n",
    "    left, right, target = [], [], []\n",
    "    for _ in range(batch_size):\n",
    "        # 일치 여부\n",
    "        res = np.random.choice([0, 1], p=[1-prob, prob])\n",
    "        # 불일치\n",
    "        if res == 0:\n",
    "            p1, p2 = tuple(np.random.choice(persons, size=2, replace=False))\n",
    "            # 폴더 내 사진이 없으면 다른 폴더 탐색\n",
    "            while len(os.listdir(os.path.join(path,p1))) < 1 or len(os.listdir(os.path.join(path,p2))) < 1:\n",
    "                p1, p2 = tuple(np.random.choice(persons, size=2, replace=False))\n",
    "            # 사진 선택\n",
    "            p1 = os.path.join(path, p1, random.choice(os.listdir(os.path.join(path,p1))))\n",
    "            p2 = os.path.join(path, p2, random.choice(os.listdir(os.path.join(path,p2))))\n",
    "            # 벡터화\n",
    "            p1 = np.expand_dims(cv2.resize(cv2.imread(p1,0), (150,150)), -1)\n",
    "            p2 = np.expand_dims(cv2.resize(cv2.imread(p2,0), (150,150)), -1)\n",
    "            \n",
    "            left.append(p1)\n",
    "            right.append(p2)\n",
    "            target.append(0)\n",
    "        # 일치\n",
    "        else:\n",
    "            p = np.random.choice(persons)\n",
    "            # 폴더 내에 2장의 사진이 없으면 다른 폴더 탐색\n",
    "            while len(os.listdir(os.path.join(path, p))) < 2:\n",
    "                p = np.random.choice(persons)\n",
    "            # 중복없이 2장의 사진 선택\n",
    "            p1, p2 = tuple(np.random.choice(os.listdir( os.path.join(path, p) ), \n",
    "                                            size=2, \n",
    "                                            replace=False ))\n",
    "            p1 = os.path.join(path, p, p1)\n",
    "            p2 = os.path.join(path, p, p2)\n",
    "\n",
    "            # 벡터화\n",
    "            p1 = np.expand_dims(cv2.resize(cv2.imread(p1,0), (150,150)), -1)\n",
    "            p2 = np.expand_dims(cv2.resize(cv2.imread(p2,0), (150,150)), -1)\n",
    "\n",
    "            left.append(p1)\n",
    "            right.append(p2)\n",
    "            target.append(1)\n",
    "\n",
    "    return [np.array(left), np.array(right)], np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(inputs, targets) = getMiniBatch(batch_size=Config.train_batch_size, path=Config.training_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 150, 150, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oneshot(model, N, path=Config.testing_dir, verbose=0):\n",
    "    \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "    if verbose:\n",
    "        pass\n",
    "        #print(\"Evaluating model on {} one-shot learning tasks ...\".format(N))\n",
    "    inputs, targets = getMiniBatch(N, path=path)\n",
    "    \n",
    "    # 모델 예측(이진분류)\n",
    "    probs = model.predict(inputs)\n",
    "    output = (np.squeeze(probs) > 0.5)*1\n",
    "    percent_correct = (output==targets).sum()*100 / N\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct, N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_init(shape, dtype=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = np.random.normal(loc=0, scale=1e-2, size=shape)\n",
    "    return K.variable(values, dtype=None)\n",
    "\n",
    "def b_init(shape, dtype=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values = np.random.normal(loc=0.5, scale=1e-2, size=shape)\n",
    "    return K.variable(values, dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 150, 150, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 150, 150, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 4096)         54019648    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " subtract (Subtract)            (None, 4096)         0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 4096)         0           ['subtract[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            4097        ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 54,023,745\n",
      "Trainable params: 54,023,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (150, 150, 1)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "#build convnet to use in each siamese 'leg'\n",
    "convnet = Sequential()\n",
    "convnet.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4),\n",
    "                   kernel_initializer=W_init))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(2e-4), \n",
    "                   kernel_initializer=W_init, bias_initializer=b_init))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4),\n",
    "                   kernel_initializer=W_init, bias_initializer=b_init))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(256, (3,3), activation='relu', kernel_regularizer=l2(2e-4),\n",
    "                   kernel_initializer=W_init, bias_initializer=b_init))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(512, (3,3), activation='relu', kernel_regularizer=l2(2e-4),\n",
    "                   kernel_initializer=W_init, bias_initializer=b_init))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(4096, activation=\"relu\", kernel_regularizer=l2(1e-3),\n",
    "                  kernel_initializer=W_init, bias_initializer=b_init))\n",
    "\n",
    "#encode each of the two inputs into a vector with the convnet\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "\n",
    "#merge two encoded inputs with the l1 distance between them\n",
    "subtracted = Subtract()([encoded_l, encoded_r])\n",
    "both = Lambda(lambda x: abs(x))(subtracted)\n",
    "prediction = Dense(1, activation='sigmoid', bias_initializer=b_init)(both)\n",
    "siamese_net = Model(inputs=[left_input, right_input], \n",
    "                    outputs=prediction)\n",
    "\n",
    "#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)\n",
    "optimizer = Adam(0.0005)\n",
    "\n",
    "#get layerwise learning rates and momentum annealing scheme described in paperworking\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델학습\n",
    "epoch = Config.train_number_epochs\n",
    "loss_every = 50\n",
    "batch_size = Config.train_batch_size\n",
    "N = 309\n",
    "best = 0\n",
    "loss_history = []\n",
    "for i in range(epoch):\n",
    "    (inputs, targets) = getMiniBatch(batch_size, path=Config.training_dir)\n",
    "    # print(inputs.shape)\n",
    "    loss = siamese_net.train_on_batch(inputs, targets)\n",
    "    loss_history.append(loss)\n",
    "    if (i+1) % loss_every == 0:\n",
    "        val_loss = siamese_net.test_on_batch(*getMiniBatch(batch_size, path=Config.validation_dir))\n",
    "        print(\"iteration {}, training loss: {:.7f}, validation loss: {:.7f}\".format(i+1, np.mean(loss_history), val_loss))\n",
    "        loss_history.clear()\n",
    "        val_acc = test_oneshot(siamese_net, N, path=Config.validation_dir, verbose=True)\n",
    "        if val_acc >= best:\n",
    "            print(\"saving\")\n",
    "            siamese_net.save_weights('saved_best')\n",
    "            best = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 62ms/step\n",
      "Got an average of 81.26009693053312% 619 way one-shot learning accuracy\n",
      "Accuracy: 81.26009693053312\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋 정확도\n",
    "siamese_net.load_weights(\"saved_best\")\n",
    "test_acc = test_oneshot(siamese_net, 619, path=Config.testing_dir, verbose=True)\n",
    "print(\"Accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 845ms/step\n",
      "0071\n",
      "0186\n"
     ]
    }
   ],
   "source": [
    "db = \"D:/data/db/\"\n",
    "face = \"D:/data/testing/0071/2.jpg\"\n",
    "real = face.split('/')[3]\n",
    "dbs = os.listdir(db)\n",
    "right = np.array([ np.expand_dims( cv2.resize( cv2.imread(os.path.join(db,person),0), (150,150) ), -1 ) for person in dbs ])\n",
    "names = [ os.path.splitext(person)[0] for person in dbs ]\n",
    "face = cv2.resize(cv2.imread(face, 0), (150,150), interpolation=cv2.INTER_AREA)\n",
    "face = np.expand_dims(face, -1)\n",
    "left = np.array([face for _ in range(len(dbs))])\n",
    "probs = np.squeeze(siamese_net.predict([left, right]))\n",
    "index = np.argmax(probs)\n",
    "prob = probs[index]\n",
    "name = \"Unknown\"\n",
    "if prob>0.5:\n",
    "    name = names[index]\n",
    "\n",
    "print(real)\n",
    "print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
